{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"},"colab":{"name":"analyze.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"round-eagle"},"source":["# Reticulating Splines\n","We are using the following packages:\n","* re: short for regular expressions, we use this to match patterns in text.\n","* copy: a helper function for getting a copy of a variable.\n","* csv: we use this to parse CSV files.\n","* nltk: short for natural language toolkit.  This provides tools for our classifier and for text preprocessing."],"id":"round-eagle"},{"cell_type":"code","metadata":{"id":"later-comedy"},"source":["import re\n","from copy import copy\n","import csv\n","\n","from nltk import NaiveBayesClassifier\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer"],"id":"later-comedy","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"responsible-spotlight"},"source":["# Opening Source\n","We begin by opening our source file, which is a CSV containing two columns.  The first column is the text of the tweet, the second column is the label.\n","\n","The syntax here may not be immediately intuitive.  The return object from the ```open()``` function is not the contents of the file as you might expect.  Instead, it returns a file object, which you can iterate over to get one line at a time.  You may need this functionality if the file is too large to fit into memory.\n","\n","In our case, it is small, so we load the whole thing using a ```for``` loop."],"id":"responsible-spotlight"},{"cell_type":"code","metadata":{"id":"designing-musician"},"source":["tweets = list()\n","file_handler = open(\"coded_tweets.csv\", encoding=\"utf8\")\n","\n","file_reader = csv.reader(file_handler)\n","next(file_reader) # The first row is the header, which we do not want, so we skip it.\n","for line in file_reader: tweets.append(line)\n","\n","file_handler.close()"],"id":"designing-musician","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"honey-removal"},"source":["train = tweets[:175]\n","test = tweets[175:]"],"id":"honey-removal","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"based-elite"},"source":["# Functions for Wrangling\n","We will apply functions to clean up our data in multiple passes.  Firstly, we will clean the whole text before it is split by spaces.  This will allow us to use regular expressions that match whole phrases.  For this we will define a ```clean_whole_text()``` function, which will take a string as its first argument and return the same string minus anything we want to clean out.\n","\n","Secondly, we define a function called ```is_valid_word()``` which takes a word as its first argument and returns ```True``` if we want to include the word in our results, or ```False``` if we want to filter it out.  This is the stage where we will remove stop words and punctuation.  We can blacklist any word we want in this function.\n","\n","Finally, we define a function called ```normalize()```, which takes a word as its first argument and returns the same word normalized.  Here is where we will do lowercasing and lemmatizing."],"id":"based-elite"},{"cell_type":"code","metadata":{"id":"finite-captain"},"source":["def clean_whole_text(text):\n","    \n","    match_patterns = [\n","        \"^RT @\\S+: \",\n","        \"@\\S+\",\n","        \"#[aA]cademic[tT]witter\",\n","        \"https?://\\S+\",\n","        \"[().,!?\\\"]\",\n","        \"&\\S+;\"\n","    ]\n","    for pattern in match_patterns:\n","        text = re.sub(pattern, \"\", text)\n","    \n","    return text"],"id":"finite-captain","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"premier-affiliate"},"source":["stop_words = stopwords.words('english')\n","\n","def is_valid_word(word):\n","    if word in stop_words: return False\n","    return True"],"id":"premier-affiliate","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"unlike-poverty"},"source":["lemmatizer = WordNetLemmatizer()\n","\n","def normalize(word):\n","    word = lemmatizer.lemmatize(word)\n","    return word.lower()"],"id":"unlike-poverty","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"straight-superior"},"source":["# Extract Features\n","This function will take a tweet string as an input and return a dict of features where the words are the keys and the values are ```True``` if the word is in the tweet, or ```False``` if it is not.\n","\n","The reason we use the ```copy()``` function here is because if we do not, Python will pass a _reference_ to the ```features_template``` variable.  This means each time this function gets called, the ```features_template``` variable will contain all the ```True``` values from the previous time the function was called.\n","\n","We take this opportunity to normalize the word and only add it to the features list if it is considered valid according to our ```is_valid_word()``` function."],"id":"straight-superior"},{"cell_type":"code","metadata":{"id":"special-doubt"},"source":["def extract_features(text):    \n","    words = text.split()\n","    features = copy(features_template)\n","    \n","    for word in words:\n","        pretty = normalize(word)\n","        if is_valid_word(pretty): features[pretty] = True\n","    \n","    return features"],"id":"special-doubt","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"finite-sullivan"},"source":["# Preparing All Features from Source\n","Here we prepare the ```features_template``` variable, which we will be our starting point for each tweet.  We first need to go through the entire dataset (including test) to capture all the words.  Then, we add each word to the ```features_template``` variable as a key with a value of ```False```. "],"id":"finite-sullivan"},{"cell_type":"code","metadata":{"id":"limiting-bandwidth"},"source":["features_template = dict()\n","for dataset in (train, test):\n","    for tweet in dataset:\n","        cleaned_tweet = clean_whole_text(tweet[0])\n","        for word in cleaned_tweet.split():\n","            pretty = normalize(word)\n","            if is_valid_word(pretty): features_template[pretty] = False"],"id":"limiting-bandwidth","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"conservative-beijing"},"source":["# Extract Features\n","The nltk classifier is not expecting a raw tweet string.  This classifier will not split or tokenize anything for us.  We need to prepare the features in advance.  The classifier ```train()``` function expects a list of lists.  The 0th index in that list must be a dictionary of features, and the 1st index must be a label.\n","\n","We go through both the training set and the testing set.  For each tweet, we clean the text using our ```clean_whole_text()``` function.  Then, we extract features and append the features with the label to the new set.\n","\n","This will transform our data from a form like this:\n","\n","```[[\"This is some tweet's text\", \"Some label\"],[\"A second tweet\", \"Another label\"]]```\n","\n","And turn it into this:\n","\n","```[[{\"some\": True, \"tweet\": True, \"text\": True, \"second\": False}, \"Some label\"],   [{\"some\": False, \"tweet\": True, \"text\": False, \"second\": True}, \"Another label]]```"],"id":"conservative-beijing"},{"cell_type":"code","metadata":{"id":"thorough-python"},"source":["train_features = list()\n","for tweet in train:\n","    cleaned_tweet = clean_whole_text(tweet[0])\n","    features = extract_features(cleaned_tweet)\n","    label = tweet[1]\n","    train_features.append((features, label))\n","    \n","test_features = list()\n","for tweet in test:\n","    cleaned_tweet = clean_whole_text(tweet[0])\n","    features = extract_features(cleaned_tweet)\n","    label = tweet[1]\n","    test_features.append((features, label))"],"id":"thorough-python","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"geographic-bruce"},"source":["# Training the Classifier\n","This ```train()``` method will return a trained classifier object.  We can take that classifier and ask it to predict a label for any other features we give it."],"id":"geographic-bruce"},{"cell_type":"code","metadata":{"id":"affected-vermont"},"source":["classifier = NaiveBayesClassifier.train(train_features)"],"id":"affected-vermont","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"caring-tactics"},"source":["# Predicting Labels"],"id":"caring-tactics"},{"cell_type":"code","metadata":{"id":"monthly-james"},"source":["for tweet in test_features:\n","    predicted_label = classifier.classify(tweet[0])\n","    print(\"Agree\" if predicted_label == tweet[1] else \"Disagree\")"],"id":"monthly-james","execution_count":null,"outputs":[]}]}